{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mou Zhang's Public-phoneme-recognition.ipynb","provenance":[{"file_id":"1Pb0kFim4CObA23_Mb4Kus5hu3I1T8q5u","timestamp":1570989412146},{"file_id":"1RcDSBzgn4wGSw055k2oXP8Zz5--QjHuT","timestamp":1569973475362}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NM1ay2pRamzJ","colab_type":"text"},"source":["<center> <h1> Phoneme Recognition </h1> </center>\n","\n","For this homework we will build a simple [Phoneme](https://https://en.wikipedia.org/wiki/Phoneme) Recognition Neural Network.\n","\n","We will use the TIMIT dataset for this homework. It containts utterances form several different English speakers saying sentences (See more details [here](https://https://catalog.ldc.upenn.edu/LDC93S1)).\n","\n","<center> <h2> Setup </h2> </center>\n","\n","#### Google colaboratory\n","\n","Before getting started, get familiar with google colaboratory:\n","https://colab.research.google.com/notebooks/welcome.ipynb\n","\n","This is a neat python environment that works in the cloud and does not require you to\n","set up anything on your personal machine\n","(it also has some built-in IDE features that make writing code easier).\n","Moreover, it allows you to copy any existing collaboratory file, alter it and share\n","with other people. In this homework, we will ask you to copy current colaboraty,\n","complete all the tasks and share your colaboratory notebook with us so\n","that we can grade it.\n","\n","#### Submission\n","\n","Before you start working on this homework do the following steps:\n","\n","1. Press __File > Save a copy in Drive...__ tab. This will allow you to have your own copy and change it.\n","2. Follow all the steps in this collaboratory file and write/change/uncomment code as necessary.\n","3. Do not forget to occasionally press __File > Save__ tab to save your progress.\n","4. After all the changes are done and progress is saved press __Share__ button (top right corner of the page), press __get shareable link__ and make sure you have the option __Anyone with the link can view__ selected.\n","5. Paste the link into your submission pdf file so that we can view it and grade."]},{"cell_type":"code","metadata":{"id":"YIccL5GzxhR2","colab_type":"code","outputId":"af670fb8-6f7e-44dd-fad9-aa9144bc41f2","executionInfo":{"status":"ok","timestamp":1571514624697,"user_tz":240,"elapsed":2302,"user":{"displayName":"Mou Zhang","photoUrl":"","userId":"02569161532640255187"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","import random\n","import torch\n","random.seed(1234)\n","torch.manual_seed(1234)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fcb928763b0>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"MQHJjzD5YAKC","colab_type":"text"},"source":["# Dataset \n","For convenience we have done some preprocessing of the TIMIT audio. In the files below, we have files `{train/dev/test}_feats.mat.npy` and `{train/dev/test}_labels.mat.npy` which contain [MFCC features](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum) and phoneme labels per frame.\n","\n","The segment below downloads the preprocessed data."]},{"cell_type":"code","metadata":{"id":"VMBltyhptzju","colab_type":"code","colab":{}},"source":["!wget -q -nc https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-phone-recognitiona/dev_feats.mat.npy\n","!wget -q -nc https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-phone-recognitiona/train_feats.mat.npy\n","!wget -q -nc https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-phone-recognitiona/test_feats.mat.npy\n","!wget -q -nc https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-phone-recognitiona/dev_labels.mat.npy\n","!wget -q -nc https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-phone-recognitiona/train_labels.mat.npy\n","!wget -q -nc https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-phone-recognitiona/test_labels.mat.npy"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7lfG4_EWclDK","colab_type":"text"},"source":["Next, we define two methods to read the numpy formatted data. `get_labels` function maps each phoneme to a index (i.e. `int`).\n","\n"]},{"cell_type":"code","metadata":{"id":"fuBERKCMwFnb","colab_type":"code","colab":{}},"source":["def get_labels(data):\n","    label_dict = {}\n","    for y in data:\n","        label_dict[y] = label_dict.get(y, len(label_dict))\n","    return label_dict\n","\n","\n","def load_npy():\n","    train_feats = np.load('train_feats.mat.npy', allow_pickle=True)\n","    train_labels = np.load('train_labels.mat.npy', allow_pickle=True)\n","    label_idx = get_labels(train_labels)\n","    test_feats = np.load('test_feats.mat.npy', allow_pickle=True)\n","    test_labels = np.load('test_labels.mat.npy', allow_pickle=True)\n","    dev_feats = np.load('dev_feats.mat.npy', allow_pickle=True)\n","    dev_labels = np.load('dev_labels.mat.npy', allow_pickle=True)\n","    return label_idx, (train_feats, train_labels), (dev_feats, dev_labels), (test_feats, test_labels)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"edcfjz_PwpSt","colab_type":"code","outputId":"efb28a6d-d464-4217-f2d3-8c027abd58be","executionInfo":{"status":"ok","timestamp":1571514674988,"user_tz":240,"elapsed":881,"user":{"displayName":"Mou Zhang","photoUrl":"","userId":"02569161532640255187"}},"colab":{"base_uri":"https://localhost:8080/","height":629}},"source":["label_dict, train, dev, test = load_npy()\n","\n","#Display the shape of the features and labels\n","print(train[0].shape, len(train[1]))\n","print(dev[0].shape, len(dev[1]))\n","print(test[0].shape, len(test[1]))\n","\n","#Display the first 40 labels\n","print(train[1][:300])\n","#Dispplay the first 40 speech features\n","print(train[0][:40])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(200000, 39) 200000\n","(10000, 39) 10000\n","(10000, 39) 10000\n","['sil' 'sil' 'sil' 'sil' 'sil' 'sil' 'sil' 'sil' 'sil' 'sil' 'sil' 'sil'\n"," 'sil' 'ax' 'ax' 'ax' 'ax' 'ax' 'ax' 's' 's' 's' 's' 's' 's' 's' 's' 's'\n"," 's' 's' 's' 's' 's' 's' 's' 's' 'uw' 'uw' 'uw' 'uw' 'uw' 'uw' 'uw' 'uw'\n"," 'uw' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'm' 'f' 'f' 'f' 'f' 'f' 'f' 'f' 'f'\n"," 'f' 'f' 'f' 'f' 'f' 'ao' 'ao' 'ao' 'ao' 'ao' 'ao' 'r' 'r' 'r' 'r' 'r'\n"," 'ix' 'ix' 'ix' 'vcl' 'vcl' 'vcl' 'vcl' 'vcl' 'z' 'z' 'z' 'z' 'z' 'z' 'z'\n"," 'z' 'ae' 'ae' 'ae' 'ae' 'ae' 'ae' 'ae' 'ae' 'ae' 'ae' 'm' 'm' 'm' 'm'\n"," 'cl' 'cl' 'cl' 'p' 'p' 'p' 'p' 'uh' 'uh' 'uh' 'l' 'l' 'l' 'l' 'l' 'l' 'l'\n"," 'l' 'l' 'l' 'l' 'l' 'l' 'ax' 'ax' 'ax' 'ax' 'ax' 'ax' 'ax' 's' 's' 's'\n"," 's' 's' 's' 's' 's' 's' 's' 's' 's' 's' 'ix' 'ix' 'ix' 'ix' 'ix' 'cl'\n"," 'cl' 'cl' 'ch' 'ch' 'ch' 'ch' 'ch' 'ch' 'ch' 'ch' 'ch' 'ch' 'ch' 'uw'\n"," 'uw' 'uw' 'uw' 'uw' 'uw' 'uw' 'uw' 'uw' 'ey' 'ey' 'ey' 'ey' 'ey' 'ey'\n"," 'ey' 'ey' 'ey' 'ey' 'sh' 'sh' 'sh' 'sh' 'sh' 'sh' 'sh' 'sh' 'sh' 'sh'\n"," 'sh' 'en' 'en' 'en' 'en' 'en' 'en' 'en' 'w' 'w' 'w' 'w' 'w' 'w' 'w' 'w'\n"," 'w' 'w' 'w' 'w' 'eh' 'eh' 'eh' 'er' 'er' 'er' 'er' 'er' 'f' 'f' 'f' 'f'\n"," 'f' 'f' 'f' 'f' 'f' 'f' 'f' 'f' 'f' 'f' 'aa' 'aa' 'aa' 'aa' 'aa' 'aa'\n"," 'aa' 'aa' 'aa' 'aa' 'aa' 'aa' 'aa' 'r' 'r' 'r' 'm' 'm' 'm' 'm' 'm' 'm'\n"," 'hh' 'hh' 'hh' 'hh' 'hh' 'hh' 'hh' 'hh' 'eh' 'eh' 'eh' 'eh' 'z' 'z' 'z'\n"," 'z' 'z' 'z' 'z' 'z' 'ax' 'ax' 'ax' 'ax' 'cl' 'cl' 'cl' 'p' 'p' 'p' 'p'\n"," 'p' 'p' 'p' 'p' 'p' 'p']\n","[[-27.69561   -21.09042     1.654011  ...   0.3484999   0.768689\n","    1.307443 ]\n"," [-28.63498   -24.11119     5.481146  ...  -0.7078144   0.1252752\n","    1.01771  ]\n"," [-31.45311   -22.93973     2.537196  ...  -1.359528   -1.21996\n","   -0.538246 ]\n"," ...\n"," [ 13.88244    -0.1123238  -9.3858    ...  -3.32069     1.63194\n","   -0.6642456]\n"," [ 14.03307     1.899275   -9.866893  ...  -3.532823    2.024883\n","   -1.48834  ]\n"," [ 14.1837      0.1750488  -8.502616  ...  -1.771908    2.117862\n","   -1.812105 ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vdMvLjPWeysE","colab_type":"text"},"source":["Every frame has a corresponding phoneme label and our goal is to train a model to predict label of unseen frames correctly.\n","\n","A simple model can predict phonemes just by considering a single frame but adding context could improve the accuracy of our model. We add context by appending each frame with neighbouring frames. After that we batch our data instances for our model to leverage the parallel processing of GPU. This is done in the function below."]},{"cell_type":"code","metadata":{"id":"xD26HuE00XT-","colab_type":"code","colab":{}},"source":["def batchify(data_feats, data_labels, batch_size, label_dict, window=5, to_cuda=False):\n","    batched_tdata = []\n","    curr_batch = []\n","    fz = np.zeros((window, 39))\n","    bz = np.zeros((window, 39))\n","    fl = ['sil'] * window\n","    bl = ['sil'] * window\n","    data_labels = fl + data_labels.tolist() + bl\n","    data_feats = np.concatenate((fz, data_feats, bz))\n","    for i in range(window, len(data_labels) - window):\n","        x = data_feats[i - window: i + window + 1]\n","        y = data_labels[i]\n","        tx = torch.Tensor(x).unsqueeze(0) # shape should be (1, 39, 2window)\n","        ty = torch.Tensor([label_dict[y]]) # shape should be (1, 1)\n","        if len(curr_batch) < batch_size:\n","            #if y != 'sil':\n","            curr_batch.append((tx, ty))\n","        else:\n","            _tx, _ty = zip(*curr_batch)\n","            b_tx = torch.cat(_tx, dim=0)\n","            b_ty = torch.cat(_ty, dim=0)\n","            if to_cuda:\n","                b_tx, b_ty = b_tx.cuda(), b_ty.cuda()\n","            batched_tdata.append((b_ty, b_tx))\n","            curr_batch = []\n","    if len(curr_batch) > 0:\n","        _tx, _ty = zip(*curr_batch)\n","        b_tx = torch.cat(_tx, dim=0)\n","        b_ty = torch.cat(_ty, dim=0)\n","        if to_cuda:\n","            b_tx, b_ty = b_tx.cuda(), b_ty.cuda()\n","        batched_tdata.append((b_ty, b_tx))\n","    return batched_tdata"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vupBGBtP06Pm","colab_type":"code","colab":{}},"source":["window=0\n","batched_train_0 = batchify(train[0], train[1], 2000, label_dict, window, True)\n","batched_dev_0 = batchify(dev[0], dev[1], 2000, label_dict, window, True)\n","batched_test_0 = batchify(test[0], test[1], 2000, label_dict, window, True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lBq9IrmQnmAP","colab_type":"text"},"source":["## Model"]},{"cell_type":"markdown","metadata":{"id":"vegGVESpg-Km","colab_type":"text"},"source":["Below we will ask you to complete the definition of a simple network. You will have to write code in parts of code where #TODO is placed. You are expected to only add code. Do not change the provided code."]},{"cell_type":"code","metadata":{"id":"5ePRzch10gII","colab_type":"code","colab":{}},"source":["class MLP_Simple(torch.nn.Module):\n","    def __init__(self,\n","                 hidden_size,\n","                 num_labels):\n","        super().__init__()\n","        # Just single frame, therefore, 1 * 39\n","        self.layer0 = torch.nn.Linear(1 * 39, hidden_size)\n","\n","        \n","        # Do not change the surrounding code, only add yours\n","        #TODO: Add more layers and activations here...\n","        self.seq = torch.nn.Sequential(\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(hidden_size, hidden_size),\n","            torch.nn.ReLU(),\n","        )\n","        self.final_layer = torch.nn.Linear(hidden_size, num_labels)\n","\n","    def forward(self, x):\n","        \"\"\"Generate output distribution and argmax\n","        Args:\n","            x: num frames of 39-dimensional feature vector (MFCC features)\n","        Return:\n","            dist: log probability for each output class\n","            pred: the label with highest log probability\n","        \"\"\"\n","        batch_size, frames, features = x.shape\n","        x = x.squeeze(1)\n","        x = self.layer0(x)\n","\n","        # TODO: use your defined layers here.\n","        x = self.seq(x)\n","      \n","\n","        y_hat = self.final_layer(x) # shape should be (batch_size, label_size)\n","        _, y_pred = y_hat.max(dim=-1) # y_pred shape should be (batch_size, 1)\n","        return y_hat, y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IWgvEGfalTuO","colab_type":"text"},"source":["Here we create an instance of our simple model that uses single frame without context."]},{"cell_type":"code","metadata":{"id":"uVg6n6LTN9-F","colab_type":"code","outputId":"f2bcbfe9-2598-491f-dc9c-7ba9c3a7d5d4","executionInfo":{"status":"ok","timestamp":1571514715870,"user_tz":240,"elapsed":1040,"user":{"displayName":"Mou Zhang","photoUrl":"","userId":"02569161532640255187"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["model_simple = MLP_Simple(hidden_size=512, num_labels=len(label_dict))\n","print(model_simple)\n","print('num parameters:', sum([p.numel() for p in model_simple.parameters()]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["MLP_Simple(\n","  (layer0): Linear(in_features=39, out_features=512, bias=True)\n","  (seq): Sequential(\n","    (0): ReLU()\n","    (1): Linear(in_features=512, out_features=512, bias=True)\n","    (2): ReLU()\n","  )\n","  (final_layer): Linear(in_features=512, out_features=48, bias=True)\n",")\n","num parameters: 307760\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F0lS_yUqnyAp","colab_type":"text"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"kKq6F0_AmRLu","colab_type":"text"},"source":["Here we define a function `train_model` that performs optimization of model's parameters. "]},{"cell_type":"code","metadata":{"id":"3tpB--OO4zr0","colab_type":"code","outputId":"a79e82ec-01ed-4cc4-feb4-a74913b3eb46","executionInfo":{"status":"ok","timestamp":1571514727665,"user_tz":240,"elapsed":8838,"user":{"displayName":"Mou Zhang","photoUrl":"","userId":"02569161532640255187"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def train_model(model, batched_train, batched_dev, batched_test, max_epoch=20):\n","    model = model.cuda()\n","    loss = torch.nn.CrossEntropyLoss(reduction='mean')\n","    optim = torch.optim.Adam(model.parameters())\n","  \n","    for epoch in range(max_epoch):\n","        random.shuffle(batched_train)\n","        train_loss = []\n","        train_acc = []\n","        model.train()\n","        for batch in batched_train:\n","            optim.zero_grad()\n","            y, x = batch\n","            y_hat, y_pred = model(x)\n","            batch_loss = loss(y_hat, y.long())\n","            batch_loss.backward()\n","            optim.step()\n","            batch_acc = (y_pred == y.long()).sum().item() / y.numel()\n","            train_loss.append(batch_loss.item())\n","            train_acc.append(batch_acc)\n","        _loss = sum(train_loss) / len(train_loss)\n","        _acc = sum(train_acc) / len(train_acc)\n","        print(f\"Epoch {epoch}\")\n","        print(f\"train loss {_loss:.4f} train_acc {_acc:.4f}\")\n","        dev_acc = []\n","        model.eval()\n","        for batch in batched_dev:\n","            y, x = batch\n","            with torch.no_grad():\n","                y_hat, y_pred = model(x)\n","                batch_acc = (y_pred == y.long()).sum().item() / y.numel()\n","                dev_acc.append(batch_acc)\n","        _acc = sum(dev_acc) / len(dev_acc)\n","        print(f\"dev_acc {_acc:.4f}\")\n","    test_acc = []\n","    model.eval()\n","    for batch in batched_test:\n","        y, x = batch\n","        with torch.no_grad():\n","            y_hat, y_pred = model(x)\n","            batch_acc = (y_pred == y.long()).sum().item() / y.numel()\n","            test_acc.append(batch_acc)\n","    _acc = sum(test_acc) / len(test_acc)\n","    print(f\"training completed.\\n\")\n","    print(f\"test_acc {_acc:.4f}\")\n","\n","train_model(model_simple, batched_train_0, batched_dev_0, batched_test_0)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 0\n","train loss 1.9320 train_acc 0.4370\n","dev_acc 0.4806\n","Epoch 1\n","train loss 1.5523 train_acc 0.5214\n","dev_acc 0.5227\n","Epoch 2\n","train loss 1.4023 train_acc 0.5616\n","dev_acc 0.5403\n","Epoch 3\n","train loss 1.3164 train_acc 0.5819\n","dev_acc 0.5441\n","Epoch 4\n","train loss 1.2548 train_acc 0.6003\n","dev_acc 0.5592\n","Epoch 5\n","train loss 1.2070 train_acc 0.6109\n","dev_acc 0.5723\n","Epoch 6\n","train loss 1.1604 train_acc 0.6228\n","dev_acc 0.5734\n","Epoch 7\n","train loss 1.1253 train_acc 0.6332\n","dev_acc 0.5856\n","Epoch 8\n","train loss 1.0928 train_acc 0.6412\n","dev_acc 0.5852\n","Epoch 9\n","train loss 1.0702 train_acc 0.6485\n","dev_acc 0.5959\n","Epoch 10\n","train loss 1.0312 train_acc 0.6603\n","dev_acc 0.5856\n","Epoch 11\n","train loss 1.0074 train_acc 0.6667\n","dev_acc 0.5841\n","Epoch 12\n","train loss 0.9903 train_acc 0.6719\n","dev_acc 0.5923\n","Epoch 13\n","train loss 0.9716 train_acc 0.6778\n","dev_acc 0.5972\n","Epoch 14\n","train loss 0.9413 train_acc 0.6861\n","dev_acc 0.6010\n","Epoch 15\n","train loss 0.9258 train_acc 0.6908\n","dev_acc 0.5848\n","Epoch 16\n","train loss 0.9037 train_acc 0.6979\n","dev_acc 0.5924\n","Epoch 17\n","train loss 0.8827 train_acc 0.7031\n","dev_acc 0.5942\n","Epoch 18\n","train loss 0.8649 train_acc 0.7084\n","dev_acc 0.5832\n","Epoch 19\n","train loss 0.8452 train_acc 0.7150\n","dev_acc 0.5946\n","training completed.\n","\n","test_acc 0.6235\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o0JrP5z0oG_G","colab_type":"text"},"source":["# Adding Context"]},{"cell_type":"markdown","metadata":{"id":"umr7Koj6oV6d","colab_type":"text"},"source":["Next, we will look at the effect of more context on model performance. The segment below creates batched data with neighboring 5 frames from left and right (of the key frame, making 11 frames in total)."]},{"cell_type":"code","metadata":{"id":"uJRf2MyroRXD","colab_type":"code","colab":{}},"source":["window= 5\n","batched_train_5 = batchify(train[0], train[1], 2000, label_dict, window, True)\n","batched_dev_5 = batchify(dev[0], dev[1], 2000, label_dict, window, True)\n","batched_test_5 = batchify(test[0], test[1], 2000, label_dict, window, True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-6lbbZ4Aon05","colab_type":"code","colab":{}},"source":["class MLP_Context(torch.nn.Module):\n","    def __init__(self,\n","                 hidden_size,\n","                 num_labels):\n","        super().__init__()\n","        self.layer0 = torch.nn.Linear(11 * 39, hidden_size)\n","\n","        \n","        # Do not change the surrounding code, only add yours\n","        #TODO: Add more layers and activations here...\n","        self.seq = torch.nn.Sequential(\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(hidden_size, hidden_size),\n","            torch.nn.ReLU()\n","        )\n","        self.final_layer = torch.nn.Linear(hidden_size, num_labels)\n","\n","    def forward(self, x):\n","        \"\"\"Generate output distribution and argmax\n","        Args:\n","            x: num frames of 39-dimensional feature vector (MFCC features)\n","        Return:\n","            dist: log probability for each output class\n","            pred: the label with highest log probability\n","        \"\"\"\n","        batch_size, frames, features = x.shape\n","        x = x.view(batch_size, -1)\n","        x = self.layer0(x)\n","\n","        # TODO: use your defined layer\n","        x = self.seq(x)\n","\n","        y_hat = self.final_layer(x) # shape should be (batch_size, label_size)\n","        _, y_pred = y_hat.max(dim=-1) # y_pred shape should be (batch_size, 1)\n","        return y_hat, y_pred"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VM2PBTqKpLE4","colab_type":"code","outputId":"2e0cdbd2-a5fc-406f-c97f-212f3e171568","executionInfo":{"status":"ok","timestamp":1571516948249,"user_tz":240,"elapsed":12920,"user":{"displayName":"Mou Zhang","photoUrl":"","userId":"02569161532640255187"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model_context = MLP_Context(hidden_size=512, num_labels=len(label_dict))\n","train_model(model_context, batched_train_5, batched_dev_5, batched_test_5)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 0\n","train loss 1.6449 train_acc 0.5205\n","dev_acc 0.5787\n","Epoch 1\n","train loss 1.1683 train_acc 0.6326\n","dev_acc 0.6188\n","Epoch 2\n","train loss 1.0177 train_acc 0.6747\n","dev_acc 0.6274\n","Epoch 3\n","train loss 0.9287 train_acc 0.6979\n","dev_acc 0.6255\n","Epoch 4\n","train loss 0.8489 train_acc 0.7201\n","dev_acc 0.6325\n","Epoch 5\n","train loss 0.7760 train_acc 0.7435\n","dev_acc 0.6419\n","Epoch 6\n","train loss 0.7127 train_acc 0.7606\n","dev_acc 0.6395\n","Epoch 7\n","train loss 0.6449 train_acc 0.7821\n","dev_acc 0.6454\n","Epoch 8\n","train loss 0.5931 train_acc 0.7981\n","dev_acc 0.6356\n","Epoch 9\n","train loss 0.5409 train_acc 0.8163\n","dev_acc 0.6262\n","Epoch 10\n","train loss 0.4939 train_acc 0.8301\n","dev_acc 0.6366\n","Epoch 11\n","train loss 0.4485 train_acc 0.8459\n","dev_acc 0.6471\n","Epoch 12\n","train loss 0.4000 train_acc 0.8621\n","dev_acc 0.6392\n","Epoch 13\n","train loss 0.3651 train_acc 0.8733\n","dev_acc 0.6263\n","Epoch 14\n","train loss 0.3206 train_acc 0.8882\n","dev_acc 0.6232\n","Epoch 15\n","train loss 0.2909 train_acc 0.8977\n","dev_acc 0.6242\n","Epoch 16\n","train loss 0.2566 train_acc 0.9094\n","dev_acc 0.6234\n","Epoch 17\n","train loss 0.2222 train_acc 0.9223\n","dev_acc 0.6225\n","Epoch 18\n","train loss 0.1948 train_acc 0.9321\n","dev_acc 0.6295\n","Epoch 19\n","train loss 0.1755 train_acc 0.9390\n","dev_acc 0.6257\n","training completed.\n","\n","test_acc 0.6580\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U-6La2_Dqm-i","colab_type":"text"},"source":["## Tasks\n","\n","\n","1. Explore with different number of layers and hidden sizes for a window size of 5.\n","\n","2. Explore different window sizes and report the one that worked best for you. Does increasing the context help?\n","\n","3. (Optional) Write code below, trying to implement model based on convolutions and see if it performs better compared to the one with fully connected layers. You might find [this](https://pytorch.org/docs/stable/nn.html?highlight=conv1d#torch.nn.Conv1d) helpful."]},{"cell_type":"code","metadata":{"id":"MIx0FOYVtZlh","colab_type":"code","colab":{}},"source":["#Cells for optional part."],"execution_count":0,"outputs":[]}]}