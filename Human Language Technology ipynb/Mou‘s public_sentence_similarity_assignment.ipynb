{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mouâ€˜s public_sentence_similarity_assignment.ipynb","provenance":[{"file_id":"1i9-uuizFuGJTzqgG_0oP3eyiJNZ-M5gE","timestamp":1575868329928},{"file_id":"1HdjpWWehlnU4VKRfOUr0KcFwfqYm6-rZ","timestamp":1574391125908},{"file_id":"1m0EA0vvJ-Ty_HraYhX72B2oigbQKm6nN","timestamp":1574382021274},{"file_id":"1AUDH_edX4RUpJfPodMpg2e9e8ARaYDaL","timestamp":1574376209248},{"file_id":"12ZXIVlHGYcyZfvWFCOdAWnYmhp-32Q8g","timestamp":1574374155561},{"file_id":"1ja2f970cI5oXsw6TtFXpyPsc5dV24m22","timestamp":1572983026641}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-KarNRHGcw0N","colab_type":"text"},"source":["<center> <h1> Transfer Learning: Sentence Similarity Task </h1> </center>\n","\n","In this assignment we will compare the performance of a LSTM based classfier to that of a Classifier built \"on top\" of the [BERT](https://arxiv.org/abs/1810.04805) sentence representation model.\n","\n","\n","We will use a subset of the [Quora duplicate question dataset](https://www.kaggle.com/c/quora-question-pairs/data). The input is a pair of questions such as:\n","\n","Q1: `Which one is more harmful to eyes CRT, TFT, LED, AMOLED, or LCD`   \n","Q2: `How do I notice whether the front panel of the TFT is LCD or LED?`   \n","Label: `0` (not similar)\n","\n","Another example:   \n","Q1:`When is the best time to take apple cider vinegar?`   \n","Q2: `How do I take Apple cider vinegar and when is the best time?`   \n","Label: `1` (similar)\n","\n","\n","As a baseline we will first construct a LSTM classifier that accepts two sequences and predicts the similarity label using the last hidden state from each sentence representation (as encoded by the LSTM). \n","\n","Next, we will use a variation of BERT known as [DistillBERT](https://arxiv.org/abs/1910.01108) and supply the two questions as one long sequence separated by a special `[SEP]` symbol. We are going to \"fine-tune\" the BERT model to perform the sentence similarty classification and predict a similar/not similar label.\n","\n","\n","## Google colaboratory\n","\n","Before getting started, get familiar with google colaboratory:\n","https://colab.research.google.com/notebooks/welcome.ipynb\n","\n","This is a neat python environment that works in the cloud and does not require you to\n","set up anything on your personal machine\n","(it also has some built-in IDE features that make writing code easier).\n","Moreover, it allows you to copy any existing collaboratory file, alter it and share\n","with other people. In this homework, we will ask you to copy current colaboraty,\n","complete all the tasks and share your colaboratory notebook with us so\n","that we can grade it.\n","\n","## Submission\n","\n","Before you start working on this homework do the following steps:\n","\n","1. Press __File > Save a copy in Drive...__ tab. This will allow you to have your own copy and change it.\n","2. Follow all the steps in this collaboratory file and write/change/uncomment code as necessary.\n","3. Do not forget to occasionally press __File > Save__ tab to save your progress.\n","4. After all the changes are done and progress is saved press __Share__ button (top right corner of the page), press __get shareable link__ and make sure you have the option __Anyone with the link can view__ selected.\n","5. Paste the link into your submission pdf file so that we can view it and grade."]},{"cell_type":"markdown","metadata":{"id":"ppQypEhWkQMw","colab_type":"text"},"source":["# Dataset\n","We have preselected a subset of the Quora duplicate question dataset and split the subset into training, validation (dev) and test sets."]},{"cell_type":"code","metadata":{"id":"6pg-lLhROCKI","colab_type":"code","outputId":"7e4e11c3-8784-4ab2-8723-27486986c1d8","executionInfo":{"status":"ok","timestamp":1579129184891,"user_tz":-480,"elapsed":4648,"user":{"displayName":"Mou Zhang","photoUrl":"","userId":"02569161532640255187"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["!wget https://raw.githubusercontent.com/jhu-intro-hlt/jhu-intro-hlt.github.io/master/data-transfer-learning-hw/{dev,test,train}.tsv -q -nc\n","!head train.tsv # display some training examples, format: sentence1,tab,sentence2,tab,label"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Can you suggest a best budget phone below 15k?\tWhat is the best phone I can buy under the price of 15000?\t1\n","How can I make a disabled or accident-prone spouse feel useful and respected?\tHow does it feel to suddenly realize that your children are the product of a broken home because of you and/or your spouse?\t0\n","I had an overdraft in Wells Fargo US! What will happen if I don't pay it?\tHow do you stop payment on a Wells Fargo check?\t0\n","What are the best places to visit this December in India?\tWhat will be the best place to visit in December in India?\t1\n","What should I do if I want to renew an expired driver's license, but had an accident while the license was expired? (India)\tIn what US state is it easy to get a driver's license?\t0\n","How/why did Stanford develop such a strong entrepreneurial culture? Why doesn't UC Berkeley have such a strong entrepreneurial culture in comparison?\tHow strong is the startup culture in Berkeley?\t0\n","How much weight can a honey bee lift?\tHow much force in Newton required to lift the water from hand pump?\t0\n","What is the scariest experience you have had on a computer?\tWhat are some of the scariest experiences that you've had?\t0\n","When is the best time to take apple cider vinegar? \tHow do I take Apple cider vinegar and when is the best time?\t1\n","How much does autopilot influence a plane? In which phase is it most used?\tHow much does the autopilot do in an airliner?\t1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JrFuULk1kj8a","colab_type":"text"},"source":["# Pretrained Models\n","We will use the [ðŸ¤— (huggingface)](https://github.com/huggingface/transformers) release of pretrained sentence representation models (Yes, it's the name of an actual [company](https://https://huggingface.co/) and they do some cool work in NLP). These can be used by first `pip install`ing the `pytorch-transformers` library. ."]},{"cell_type":"code","metadata":{"id":"igc-NhlYOc7s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"8bb0c685-ab84-489e-aaa1-0f6007660367","executionInfo":{"status":"ok","timestamp":1579129190080,"user_tz":-480,"elapsed":9832,"user":{"displayName":"Mou Zhang","photoUrl":"","userId":"02569161532640255187"}}},"source":["!pip install pytorch-transformers -q # install python library for pretrained BERT (and other similar) models"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184kB 3.4MB/s \n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 870kB 49.7MB/s \n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0MB 46.3MB/s \n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GCrnza0SQcnM","colab_type":"code","colab":{}},"source":["import torch\n","import random\n","import time\n","import math\n","from pytorch_transformers import DistilBertModel as BertModel\n","from pytorch_transformers import DistilBertTokenizer as BertTokenizer\n","random.seed(1234)\n","torch.manual_seed(1234)\n","torch.cuda.set_device(0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-7Vcp-d0nxVd","colab_type":"text"},"source":["# Data Reader\n","The `STSCorpus` (SenTence Similarity) class handles the data loading, processing and itertating (during training and testing). It accepts a flag `bert_format` that preprocessing the data either using a standard format (reading words and converting them to integers) or a bert format which uses `DistilBertTokenizer` provided in the pytorch-transformer library. "]},{"cell_type":"code","metadata":{"id":"ulIMqtG1M_6l","colab_type":"code","colab":{}},"source":["SPL_SYMS = ['<PAD>','<BOS>', '<EOS>', '<UNK>']\n","\n","\n","class STSCorpus(object):\n","  def __init__(self,\n","              file,\n","              vocab=None,\n","              cuda=False,\n","              batch_size=1, bert_format=0):\n","    self.bert_format = bert_format\n","    if self.bert_format == 0:\n","      self.bert_tokenizer = None\n","      self.max_vocab = 64000\n","    else:\n","      self.bert_tokenizer = BertTokenizer.from_pretrained('distilbert-base-uncased')\n","      self.max_vocab = self.bert_tokenizer.vocab_size\n","    self.max_size = 0\n","    self.batch_size = batch_size\n","    self.vocab = self.make_vocab(file, vocab)\n","    self.idx2vocab = self.make_idx2vocab(self.vocab)\n","    self.data = self.numberize(file, self.vocab, cuda)\n","    self.batch_data = self.batchify()\n","    self.data_size = len(self.batch_data)\n","\n","  def batchify(self,):\n","    self.batch_data = []\n","    curr_batch = []\n","    max_x1, max_x2 = 0, 0\n","    for x1, x2, y in self.data:\n","      if len(curr_batch) < self.batch_size:\n","        curr_batch.append((x1, x2, y))\n","        max_x1 = max(max_x1, x1.shape[1])\n","        if self.bert_format == 0:\n","          max_x2 = max(max_x2, x2.shape[1]) \n","      else:\n","        \n","        _x1, _x2, _y = zip(*curr_batch)\n","        \n","        \n","        if self.bert_format == 0:\n","          _x1 = [torch.cat((torch.zeros(1, max_x1 - i.shape[1]).type_as(i), i), dim=1) for i in _x1]\n","          batch_x1 = torch.cat(_x1, dim=0)\n","          _x2 = [torch.cat((torch.zeros(1, max_x2 - i.shape[1]).type_as(i), i), dim=1) for i in _x2]\n","          batch_x2 = torch.cat(_x2, dim=0) if _x2[0] is not None else None\n","        else:\n","          _x1 = [torch.cat((i, torch.zeros(1, max_x1 - i.shape[1]).type_as(i)), dim=1) for i in _x1]\n","          batch_x1 = torch.cat(_x1, dim=0)\n","          batch_x2 = None\n","        batch_y = torch.cat(_y, dim=0)\n","        self.batch_data.append((batch_x1, batch_x2, batch_y))\n","        curr_batch = []\n","        max_x1, max_x2 = 0, 0\n","    # remaining items in curr_batch\n","    if len(curr_batch) > 0:\n","      print(len(self.batch_data),  max_x1, max_x2)\n","      _x1, _x2, _y = zip(*curr_batch)\n","      \n","      \n","      if self.bert_format == 0:\n","        _x1 = [torch.cat((torch.zeros(1, max_x1 - i.shape[1]).type_as(i), i), dim=1) for i in _x1]\n","        batch_x1 = torch.cat(_x1, dim=0)\n","        _x2 = [torch.cat((torch.zeros(1, max_x2 - i.shape[1]).type_as(i), i), dim=1) for i in _x2]\n","        batch_x2 = torch.cat(_x2, dim=0) if _x2[0] is not None else None\n","      else:\n","        _x1 = [torch.cat((i, torch.zeros(1, max_x1 - i.shape[1]).type_as(i)), dim=1) for i in _x1]\n","        batch_x1 = torch.cat(_x1, dim=0)\n","        batch_x2 = None\n","      batch_y = torch.cat(_y, dim=0)\n","      self.batch_data.append((batch_x1, batch_x2, batch_y))\n","    return self.batch_data\n","\n","  def numberize(self, txt, vocab, cuda=False):\n","    data = []\n","    max_size = 0\n","    with open(txt, 'r', encoding='utf8') as corpus:\n","      for l in corpus:\n","        l1, l2, y = l.split('\\t')\n","        y = torch.Tensor([[float(y)]]).float()\n","        if self.bert_format == 0:\n","          d1 = [vocab['<BOS>']] + [vocab.get(t, vocab['<UNK>']) for t in l1.strip().split()] + [vocab['<EOS>']]\n","          d1 = torch.Tensor(d1).long()\n","          d1 = d1.unsqueeze(0) # shape = (1, N)\n","          d2 = [vocab['<BOS>']] + [vocab.get(t, vocab['<UNK>']) for t in l2.strip().split()] + [vocab['<EOS>']]\n","          d2 = torch.Tensor(d2).long()\n","          d2 = d2.unsqueeze(0) # shape = (1, N)\n","          max_size = max(d1.shape[1], d2.shape[1], max_size)\n","          if cuda:\n","            d1 = d1.cuda()\n","            d2 = d2.cuda()\n","            y = y.cuda()\n","        elif self.bert_format == 1:\n","          _d1 = torch.Tensor(self.bert_tokenizer.encode(\"[CLS] \" + l1 + \" [SEP]\")).long()\n","          _d2 = torch.Tensor(self.bert_tokenizer.encode(\" \" + l2 + \" [SEP]\")).long()\n","          d = torch.cat([_d1, _d2], dim=0).unsqueeze(0)\n","          max_size = max(d.shape[1], max_size)\n","          if cuda:\n","            d1 = d.cuda()\n","            d2 = None\n","            y = y.cuda()\n","        else:\n","          pass\n","        data.append((d1, d2, y))\n","    self.max_size = max_size\n","    return data\n","\n","  def make_idx2vocab(self, vocab):\n","    if vocab is not None:\n","      idx2vocab = {v: k for k, v in vocab.items()}\n","      return idx2vocab\n","    else:\n","      return None\n","\n","  def make_vocab(self, txt, vocab):\n","    if vocab is None and txt is not None:\n","      vc = {}\n","      for line in open(txt, 'r', encoding='utf-8').readlines():\n","        x1, x2, y = line.strip().split('\\t')\n","        for w in x1.split() + x2.split():\n","          vc[w] = vc.get(w, 0) + 1\n","      cv = sorted([(c, w) for w, c in vc.items()], reverse=True)\n","      cv = cv[:self.max_vocab]\n","      _, v = zip(*cv)\n","      v = SPL_SYMS + list(v)\n","      vocab = {w: idx for idx, w in enumerate(v)}\n","      return vocab\n","    else:\n","      return vocab\n","\n","  def get(self, idx):\n","    return self.batch_data[idx]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zvu6zsZ-pgZA","colab_type":"text"},"source":["Creating train, dev and test data objects. (with `bert_format=0`) and places the data on the GPU."]},{"cell_type":"code","metadata":{"id":"Kk05u089ZmVC","colab_type":"code","outputId":"a19a4e5d-ea7e-4fde-84e3-af942ecb3b76","executionInfo":{"status":"ok","timestamp":1579129213811,"user_tz":-480,"elapsed":33516,"user":{"displayName":"Mou Zhang","photoUrl":"","userId":"02569161532640255187"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["train_corpus = STSCorpus(file='train.tsv',\n","                         cuda=True,\n","                         batch_size=32, \n","                         bert_format=0)\n","dev_corpus = STSCorpus(file='dev.tsv', vocab=train_corpus.vocab,\n","                       cuda=True,\n","                       batch_size=32, \n","                       bert_format=0)\n","test_corpus = STSCorpus(file='test.tsv', vocab=train_corpus.vocab,\n","                        cuda=True,\n","                        batch_size=1,\n","                        bert_format=0)\n","print(train_corpus.data_size, dev_corpus.data_size, test_corpus.data_size)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["1212 19 15\n","151 30 23\n","1213 152 2500\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xlkzq83Uzips","colab_type":"text"},"source":["the training input batch looks like this. The input is formated as a tuple the first item is a batch of Q1s (once they are converted to integers) and the second item is a batch of Q2s."]},{"cell_type":"code","metadata":{"id":"0RxnLgrDybnZ","colab_type":"code","outputId":"4363ecce-6f76-416a-efb0-cf6c3b3b2292","executionInfo":{"status":"ok","timestamp":1579129213812,"user_tz":-480,"elapsed":33503,"user":{"displayName":"Mou Zhang","photoUrl":"","userId":"02569161532640255187"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["print(train_corpus.batch_data[0][:2])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     1,    48,    17,  1022,     7,    23,\n","          1199,   166,  1932,  6382,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             1,    13,    18,     9,    76,     7,  4012,    26, 45538,  2312,\n","           144,  1052,    12, 33066,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     1,     9,   171,\n","            32, 13296,     8, 14337, 18610, 46694,     5,    39,   149,    35,\n","             9,    96,   468,   111,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     1,     5,    14,     4,    23,   241,     6,   249,\n","            91,  2087,     8,    64,     2],\n","        [    1,     5,    38,     9,    15,    35,     9,   105,     6, 11283,\n","            32,  5460,  6182, 21845,   118,   171,    32, 12094,   178,     4,\n","          1515,    75, 40327, 11019,     2],\n","        [    0,     0,     1, 52732,    74,  2612,   856,   381,     7,  1591,\n","         11767,  1964,    22,   304,  3749, 10951,    33,   381,     7,  1591,\n","         11767,   389,     8, 23534,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     1,    13,    81,   334,    18,\n","             7,  3994, 12008,  6545,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     1,     5,    10,     4,  3945,   519,    17,    33,\n","           171,    19,     7,  1267,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     1,   164,    10,     4,    23,   107,     6,\n","           120,  3076,  5823,  8059,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     1,\n","            13,    81,    31, 17556,  3141,     7,  4129,   184,    99,  2064,\n","            10,    20,    55,  1523,     2],\n","        [    0,     0,     0,     0,     0,     0,     1,    49,   230,  7587,\n","            52,    67,    12,   395,    45,    39,    25,   176,   367,    19,\n","             4,  1156,    11,   624,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     1,   114,   604,  5505,     8,  4757,\n","            16, 13629,  7802,  1752,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     1,    13,   394,    10,    20,     6,    29,   104, 46702,\n","            16,     4,  1474,  8742,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     1,    27,    20,   123,     6,   245,   538,  4052,   219,\n","            41,   619,    41, 17576,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     1, 25186,     5,    14,     4,   928,     8,    99,  1240,\n","         39095,   944,   128,  1182,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     1,     5,    14,     4,    23,  3689,   193,    24,    18,\n","           154,    28,  5240,  2544,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     1,     5,    14,     4,    55,  1795,  1916,    16,    95,\n","             6,   227,    21,   711,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     1,     5,    31,    20,   144,    50,     6,    33,    32,\n","         13326,   664,   261,  2058,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     1,    13,    10,     4,  7911,   209,\n","         47467, 57150,  1360,  1107,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             1,    37,    58,    10,    97,  4158,     6,  1518, 55579, 47299,\n","         51584, 57199,    26, 51588,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     1,\n","            27,    20,   300,    16,    95,    26,    21,   633,     6,   213,\n","           630,   372,    81, 14278,     2],\n","        [    0,     0,     0,     0,     1,  3522,  5556,   505,    17,   110,\n","          3412,    19,     7,   888,   977,   894,     8,  3930,  6732,  4224,\n","             5,   251,    20,  2933,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     1,    13,    15,     9,    29,\n","          9548, 44576,    93, 36628,     2],\n","        [    0,     0,     0,     0,     0,     0,     1,     5,    14,     4,\n","            55,   722,  1294,     9,   159,     8,   421,     6,   457,    21,\n","           329,   255,    42,   914,     2],\n","        [    0,     0,     1,     9,   105,     6,   157,     4, 24825, 52715,\n","          4080, 27128, 12305, 15034, 48291,  5466, 19471,    71,    18,     9,\n","           102,    20,     8,    64,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             1,   258,     7,  3242,  4228, 13163,  2946,    25,    54,    16,\n","          3057, 22180,   594,  8204,     2],\n","        [    0,     0,     1,    37,    14,     4,    55,  1416,  2055,  2729,\n","             9,    38,   390,   153,     9,  1148,    12,    53,    15,     9,\n","           390,   278,   179,   279,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     1,\n","            22,    15,    40,   742,     6,   124,  2573,    84,    19,    73,\n","           911,    80,  4000,   111,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     1,   184, 28102, 36155,    11, 53112,    86, 36145,    11,\n","          9636,  4046,    10,  1234,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     1,    37,    58,    10,   119,  6091,  8384,\n","          2647,    26, 16302, 57680,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     1,\n","             5,    14,   302,  1290,  3605,  2730,    47,   492,  1606,  1318,\n","             6,     4,  3930, 42509,     2],\n","        [    0,     0,     0,     0,     1,    13,    81,    31,    20,   360,\n","            12,   259,    18,     9,  1810,     7, 31836,   903,    16,     7,\n","           951,   373,  3271,  7378,     2]], device='cuda:0'), tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             1,     5,    10,     4,    23,   166,     9,    18,   157,   261,\n","             4,   812,    11,  6383,     2],\n","        [    1,    13,    31,    20,   144,     6,  1775,  2521,    24,    36,\n","          1069,    14,     4,   835,    11,     7,  2817,   510,   428,    11,\n","            17,  1544,    36,  7485,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     1,    13,    15,    17,   155,  2098,    19,\n","             7, 14337, 18610,  7778,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     1,     5,    39,    25,     4,    23,   341,     6,   249,\n","             8,  2087,     8,    64,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     1,   184,    45,   131,   532,    10,    20,   882,     6,\n","            29,     7,  6182,  4483,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     1,    13,  1591,    10,     4,\n","           700,   389,     8,  6829,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             1,    13,    81,  1333,     8, 12286,   603,     6,  3037,     4,\n","           355,    34,   997, 11337,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     1,     5,    14,    30,    11,     4,  3945,\n","          2285,    24,   499,  3467,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             1,    13,    15,     9,   120,   578,  5823,  3778,    12,    46,\n","            10,     4,    23,   204,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     1,    13,    81,    31,     4, 17556,\n","            15,     8,    32, 14272,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     1,    13,    39,\n","             4,  1889,   534,    25,   710,    35,    44,    10,     7,   230,\n","            52,    67,    12,   564,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     1,     5,    14,     4,    23,   567,    28,   265,   662,\n","            11,  5715,  1361,  8270,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     1,    13,   394,    43,    20,    25,     6,    29,   104,\n","             4,  1474,  1939, 46701,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     1,\n","             9,   105,     6,    15,   538,  4052,    12, 44494,     8,     7,\n","         27439,    27,    20,  1293,     2],\n","        [    0,     0,     0,     0,     0,     0,     1, 25186,     5,    14,\n","            30,    11,     4,   944, 44770,    24, 12240,  4409, 45917, 45794,\n","            10,  1240,   356,  1182,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     1,     5,    14,    30,  3689,   193,    11,  1497,\n","            90,     6,  5240, 34206,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     1,     5,    38,     9,    15,     6,\n","           227,    21,  2418,   711,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     1,     5,    10,    20,    50,     6,\n","            33,    32, 14209,  2340,     2],\n","        [    0,     0,     1,   239, 26112,  1360,  2774,   208,   445,    10,\n","         27903, 28323, 28035,    13,    18,     9,    29, 27898,  1153,   346,\n","             6,   151,    16, 26562,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     1,    13,\n","            15,     9,  4286,   861,     4,   799,   611,    11,     4, 47300,\n","            10,  8705,    26, 51583,     2],\n","        [    0,     0,     0,     1,     9,   165,    21,   633,   372,    81,\n","            12,  7456,   213,   237, 21957, 42452,    13,    18,     9,   100,\n","             6,   165,   237,  7047,     2],\n","        [    0,     0,     0,     0,     1,  3522,  5556,   505,    17,   110,\n","          3412,    19,     7,   888,   355,   894,     8,  3930,  6732,  4224,\n","             5,   251,    20,  2933,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             1,    37,    10,     4,  5680,   766,  1560,    16, 22473,    11,\n","          1536,    12,  1985,  1107,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             1,     5,  1327,  1294,    43,     9,   159,     6,   457,     7,\n","         22533,   510,  3154,  1267,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     1,    13,    86, 13297,    31, 12305,\n","         15034,    33,     8,    64,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     1,\n","           258,     7,  3242, 13163,  4228,  3154,  2946,    25,    54,    16,\n","          3057, 22180,   594,  8204,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     1,     5,    14,    30,    11,\n","             4,    23,  2055,   993,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     1,    22,    15,\n","            40,   124,    84,    19,    73,    24,    14,   298,     6,   102,\n","           307,   372,    19,   506,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     1,    13,    86,  7818,    14,  4619,\n","             8,    32,  2897, 14159,     2],\n","        [    0,     0,     1,   170,  1012,   299,   168,  1973,   239,  1494,\n","            10,  4840, 21772,    48,     9, 11805,    21,  1494,   212,    63,\n","         31792,  4340,    26,  2932,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     1,     5,    14,   302,  1290,  3605,  2730,    47,   492,\n","          1606,  1318,     6, 19584,     2],\n","        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             1,    13,    81,    31,    20,   360,     6,    29,    36,  7454,\n","         21066,     8,  3271,  7378,     2]], device='cuda:0'))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6YhtJJoHz6gP","colab_type":"text"},"source":["the training output (i.e. desired predictions) batch looks like this:"]},{"cell_type":"code","metadata":{"id":"FGkd0qTUy09o","colab_type":"code","outputId":"0ae4bc34-85b9-4b09-f387-183a3d58f43d","executionInfo":{"status":"ok","timestamp":1579129213813,"user_tz":-480,"elapsed":33491,"user":{"displayName":"Mou Zhang","photoUrl":"","userId":"02569161532640255187"}},"colab":{"base_uri":"https://localhost:8080/","height":561}},"source":["print( train_corpus.batch_data[0][2])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["tensor([[1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [0.]], device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6ber1qV5pxJ_","colab_type":"text"},"source":["# Training Routine"]},{"cell_type":"code","metadata":{"id":"U1265VP3bBrt","colab_type":"code","colab":{}},"source":["def train(model, train_cropus, dev_corpus, max_epochs):\n","  sum_loss, sum_acc = 0., 0.\n","  train_instances_idxs = list(range(train_corpus.data_size))\n","  st = time.time()\n","  for epoch_i in range(max_epochs):\n","    sum_loss, sum_acc = 0., 0.\n","    random.shuffle(train_instances_idxs)\n","    model.train()\n","    for i in train_instances_idxs:\n","      x1, x2, y = train_corpus.get(i)\n","      l, a = model.train_step(x1, x2, y)\n","      sum_loss += l\n","      sum_acc += a\n","    print(f\"epoch: {epoch_i} time elapsed: {time.time() - st:.2f}\")\n","    print(f\"train loss: {sum_loss/train_corpus.data_size:.4f} train acc: {sum_acc/train_corpus.data_size:.4f}\")\n","    sum_loss, sum_acc = 0., 0.\n","    model.eval()\n","    for dev_i in range(dev_corpus.data_size):\n","      x1, x2, y = dev_corpus.get(dev_i)\n","      with torch.no_grad():\n","        l, a = model(x1, x2, y)\n","        sum_loss += l\n","        sum_acc += a\n","    print(f\"  dev loss: {sum_loss/dev_corpus.data_size:.4f}   dev acc: {sum_acc/dev_corpus.data_size:.4f}\")\n","  return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q62YvNKlp1VA","colab_type":"text"},"source":["# Evaluation Routine"]},{"cell_type":"code","metadata":{"id":"IPlEx7Akb7m8","colab_type":"code","colab":{}},"source":["def evaluate(model, test_corpus):\n","  print('Predictions:')\n","  sum_acc = 0.0\n","  model.eval()\n","  for test_i in range(test_corpus.data_size):\n","    x1, x2, y = test_corpus.get(test_i)\n","    _, pred = model.predict(x1, x2)\n","    sum_acc += (1 if pred.item() == y.item() else 0)\n","  print(f\"Avg acc: {sum_acc/test_corpus.data_size:.4f}\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"68Sw26rjp4nn","colab_type":"text"},"source":["#Part 1: Baseline Classifier\n","In the first part of the assignment you will complete the code for a baseline classifier which uses simple LSTM-RNNs to encoder a pair of sentences. The last time-step hidden state is then used to predict if the two sentences are similar or not. If you are unfamiliar with LSTMs [this](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) is an excellent resource. Note: you don't have to memorize the internals of an LSTM, for this assignment just knowing that LSTMs expect three inputs 1. a representation of a word 2. previous hidden state and 3. the previous cell state is sufficient. In pytorch LSTMs \"wrap\" the hidden state (lets call it h) and the cell state (lets call it c) into a tuple (h,c)."]},{"cell_type":"code","metadata":{"id":"avT4K1diMcvS","colab_type":"code","colab":{}},"source":["class Classifier(torch.nn.Module):\n","    def __init__(self,\n","                 vocab_size,\n","                 embedding_size,\n","                 hidden_size,\n","                 num_layers=1,\n","                 dropout=0.1,\n","                 max_grad_norm=5.0):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.embedding_size = embedding_size\n","        self.max_grad_norm = max_grad_norm\n","        #TODO: create a drouput layer. Use the `dropout` value from the `__init__` arguments.\n","        self.dropout_layer = torch.nn.Dropout(p = dropout)\n","        \n","        if max(vocab_size,embedding_size ,hidden_size,num_layers) > 0:\n","          #TODO: create an embedding layer here\n","          #TODO: the embedding layer takes a sequence of ints and converts them into a sequence of real-valued vectors\n","          #TODO: see https://pytorch.org/docs/stable/nn.html?highlight=embedding#torch.nn.Embedding\n","          self.embedding_layer = torch.nn.Embedding(num_embeddings = vocab_size, embedding_dim = self.embedding_size)\n","          \n","          #TODO: create a unidirectional RNN-LSTM here, Note: Set `batch_first=True`\n","          #TODO: see https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM\n","          self.uni_RNN_LSTM_layer = torch.nn.LSTM(input_size = self.embedding_size, hidden_size = self.hidden_size, num_layers=self.num_layers,  dropout = dropout, batch_first= True)\n","          #TODO: create a Linear layer that takes 2 * hidden_size and outputs a single output (binary label)\n","          #TODO: name this layer as self.output\n","          #TODO: https://pytorch.org/docs/stable/nn.html?highlight=linear#torch.nn.Linear\n","          self.output = torch.nn.Linear(in_features=self.hidden_size * 2, out_features = 1)\n","          \n","\n","          #we will package the optimier inside the model class for convenience.\n","          self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()))\n","        else:\n","          pass\n","          \n","        #TODO: create a Binary Cross Entropy loss object here, set reduction='mean'\n","        #TODO: name it `self.loss`\n","        self.loss = torch.nn.BCELoss(reduction='mean')\n","          \n","\n","    def predict(self, x1, x2):\n","        \"\"\" Generates a prediction and probability for each input instance\n","        Args:\n","            x1: sequence of input tokens for the first sentence\n","            x2: sequence of input tokens for the second sentence\n","        Returns:\n","            out: sequence of output predictions (probabilities) for each instance\n","            pred: the discrete prediction from the output probabilities\n","        \"\"\"\n","        batch_size, seq_len = x1.shape\n","        batch_size2, seq_len2 = x2.shape\n","        assert batch_size == batch_size2\n","        \n","        #TODO: embed the x1 sequence into a sequence of embeddings, then apply dropout\n","        #TODO: name the result `emb_x1`\n","        emb_x1 = self.dropout_layer(self.embedding_layer(x1))\n","        \n","        #TODO: embed the x2 sequence into a sequence of embeddings, then apply dropout\n","        #TODO: name the result `emb_x2`\n","        emb_x2 = self.dropout_layer(self.embedding_layer(x2))\n","        \n","        #TODO: create an initial state (hidden and cell states) of zeros for the LSTM, this should support batching and num_layers>1\n","        h, c = (torch.zeros(self.num_layers, batch_size, self.hidden_size).cuda(),\n","                torch.zeros(self.num_layers, batch_size, self.hidden_size).cuda())\n","        #TODO: use the LSTM to get the hidden states of the last time-step of the x1 sequence\n","        x1_out, (x1_hidden, x1_cell) = self.uni_RNN_LSTM_layer(emb_x1, (h, c))\n","        #TODO: use the LSTM to get the hidden states of the last time-step of the x2 sequence\n","        x2_out, (x2_hidden, x2_cell) = self.uni_RNN_LSTM_layer(emb_x2, (h, c))\n","        #TODO: concat the last time-step hidden states from the two sequences\n","        #TODO: name the concated result `final_hidden`\n","        #TODO: `final_hidden` should have shape (batch_size, 2 * hidden_size)\n","        final_hidden = torch.cat((x1_out[:,-1,:].squeeze(1), x2_out[:,-1,:].squeeze(1)), -1)\n","        #TODO: apply dropout to the `final_hidden` tensor\n","        final_hidden = self.dropout_layer(final_hidden)\n","        #TODO: pass `final_hidden` throught the `self.output` linear layer and then\n","        #TODO: apply a sigmoid transformation to the output of self.output\n","        #TODO: name the transformed output as `out`\n","        #TODO: `out` should have the shape (batch_size, 1)\n","        out = torch.sigmoid(self.output(final_hidden))\n","\n","        pred = out.clone().detach()\n","        pred[pred >= 0.5] = 1\n","        pred[pred < 0.5] = 0\n","        return out, pred\n","\n","    def forward(self, x1, x2, y):\n","        \"\"\"Generates the loss and accuracy given a batch of sequences x1 and x2 and their associated classification label y\n","        Args:\n","            x1: sequence of indexes representing the first sentence\n","            x2: sequence of indexes representing the second sentence\n","            y: binary valued tensor representing the label for each x1,x2 sentence pair\n","        Returns:\n","            loss: the Binary cross entropy loss from the current batch of x1, x2, y\n","            acc: the accuracy of the current-batch predictions\n","        \"\"\"\n","        #TODO: use the `self.predict` function to get the `out` and `pred`\n","        out, pred = self.predict(x1,x2)\n","        #TODO: compute the loss using the output (from the previous line and the labels `y`\n","        loss = self.loss(out, y)\n","\n","        assert pred.shape == y.shape\n","        acc = (pred == y).sum().item() / y.numel()\n","        return loss, acc\n","\n","    def train_step(self, x1, x2, y):\n","        \"\"\" Performs one step of SGD\n","        Args:\n","            x1: the input sequence, its size should be: (1, x1_length)\n","            x2: the input sequence, its size should be: (1, x2_length)\n","            y: the output label, its size should be (1, 1)\n","        Returns:\n","            loss: the loss for this example (note this is just for logging it is not a pytorch tensor)\n","            accuracy: the accuracy for this example\n","        \"\"\"\n","        self.optimizer.zero_grad()\n","        _loss, acc = self(x1, x2, y) # calls self.forward(x, y)\n","        _loss.backward()\n","        grad_norm = torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, self.parameters()),\n","                                                   self.max_grad_norm)\n","\n","        if math.isnan(grad_norm):\n","            print('skipping update grad_norm is nan!')\n","        else:\n","            self.optimizer.step()\n","        loss = _loss.item()\n","        return loss, acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9FjjFilmaxeT","colab_type":"code","outputId":"cbb54905-0146-4f1a-e523-34381c7f268f","executionInfo":{"status":"ok","timestamp":1579129214759,"user_tz":-480,"elapsed":34392,"user":{"displayName":"Mou Zhang","photoUrl":"","userId":"02569161532640255187"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["base_model = Classifier(vocab_size=len(train_corpus.vocab),\n","                        embedding_size=1024,\n","                        hidden_size=1024,\n","                        num_layers=2)\n","print(base_model, '\\ncontains', sum([p.numel() for p in base_model.parameters() if p.requires_grad]), 'parameters')\n","base_model = base_model.cuda()"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Classifier(\n","  (dropout_layer): Dropout(p=0.1, inplace=False)\n","  (embedding_layer): Embedding(61585, 1024)\n","  (uni_RNN_LSTM_layer): LSTM(1024, 1024, num_layers=2, batch_first=True, dropout=0.1)\n","  (output): Linear(in_features=2048, out_features=1, bias=True)\n","  (loss): BCELoss()\n",") \n","contains 79858689 parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z48mQ5YNeJGS","colab_type":"code","outputId":"fcde32a7-c58a-4da5-9b69-f14014b2c158","executionInfo":{"status":"ok","timestamp":1579129775083,"user_tz":-480,"elapsed":594703,"user":{"displayName":"Mou Zhang","photoUrl":"","userId":"02569161532640255187"}},"colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["base_model = train(base_model, train_corpus, dev_corpus, 5)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["epoch: 0 time elapsed: 106.63\n","train loss: 0.5645 train acc: 0.6994\n","  dev loss: 0.5138   dev acc: 0.7454\n","epoch: 1 time elapsed: 219.17\n","train loss: 0.4056 train acc: 0.8183\n","  dev loss: 0.5149   dev acc: 0.7667\n","epoch: 2 time elapsed: 331.78\n","train loss: 0.2191 train acc: 0.9097\n","  dev loss: 0.6921   dev acc: 0.7673\n","epoch: 3 time elapsed: 444.53\n","train loss: 0.0916 train acc: 0.9650\n","  dev loss: 0.9610   dev acc: 0.7561\n","epoch: 4 time elapsed: 557.07\n","train loss: 0.0557 train acc: 0.9800\n","  dev loss: 1.3052   dev acc: 0.7552\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OQRf1DcDTv1U","colab_type":"code","outputId":"488aa4db-a650-408a-ceab-907f42d330cd","executionInfo":{"status":"ok","timestamp":1579129792139,"user_tz":-480,"elapsed":611746,"user":{"displayName":"Mou Zhang","photoUrl":"","userId":"02569161532640255187"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["evaluate(base_model, test_corpus)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Predictions:\n","Avg acc: 0.7760\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rk-B5fNvu5pZ","colab_type":"text"},"source":["Creating train, dev and test data objects. (with `bert_format=1`)."]},{"cell_type":"code","metadata":{"id":"yNePb0x4f5Dg","colab_type":"code","outputId":"56b67ac9-76c8-43f9-df78-16970baa6a11","executionInfo":{"status":"ok","timestamp":1579129829324,"user_tz":-480,"elapsed":648916,"user":{"displayName":"Mou Zhang","photoUrl":"","userId":"02569161532640255187"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["train_corpus = STSCorpus(file='train.tsv',\n","                          cuda=True,\n","                          batch_size=32, bert_format=1)\n","dev_corpus = STSCorpus(file='dev.tsv', vocab=train_corpus.vocab,\n","                        cuda=True,\n","                        batch_size=32,bert_format=1)\n","test_corpus = STSCorpus(file='test.tsv', vocab=train_corpus.vocab,\n","                        cuda=True,\n","                        batch_size=1,bert_format=1)\n","print(train_corpus.data_size, dev_corpus.data_size, test_corpus.data_size)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 231508/231508 [00:00<00:00, 2652872.30B/s]\n"],"name":"stderr"},{"output_type":"stream","text":["1212 33 0\n","151 51 0\n","1213 152 2500\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zVai29qau8tl","colab_type":"text"},"source":["# Part 2: BERT based Classifier\n","Next we will implement a sentence similarity predictor using DistilBERT. A nice property/design of pytorch-transformer library is that we can obtain a pretrained BERT model using the following simple line of code:"]},{"cell_type":"code","metadata":{"id":"6eiSbAgFMfoL","colab_type":"code","colab":{}},"source":["class BERTClassifier(Classifier):\n","    def __init__(self,\n","                 dropout=0.1,\n","                 max_grad_norm=5.0):\n","        super().__init__(0, 0, 0, 0, dropout, max_grad_norm)\n","        self.output = torch.nn.Linear(768, 1)\n","        #TODO: we have created a linear layer `self.output` for you.\n","        #TODO: for Bert fine-tuning to work, the weights of this layer should be initialized to a small random values\n","        #TODO: initialize the `weight` variable in self.output \n","        #TODO: with a 0 mean 0.05 var Normal distribution\n","        #TODO: this link may be useful https://pytorch.org/cppdocs/api/function_namespacetorch_1_1nn_1_1init_1a105c2a8ef81c6faa82a01cf35ce9f3b1.html\n","        weight = torch.nn.init.normal_(torch.zeros(1,768), mean = 0, std = 0.05)\n","        self.output.weight = torch.nn.Parameter(weight)\n","        #using a pretrained bert model using pytorch-transformers is as easy as adding the line below!\n","        self.bert_model = BertModel.from_pretrained('distilbert-base-uncased')\n","        #note that the learning rate for fine-tuning should be small, we will use 1e-5 for our learning rate.\n","        self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=1e-5)\n","\n","    def predict(self, x1, x2=None):\n","        assert x2 is None\n","        #TODO: x1 is a batch of sequence pairs. \n","        #TODO: BERT (and DistillBERT) have been trained such that\n","        #TODO: the ouput at the first time-step can be used for sentence similarity classification.\n","        #TODO: Pass the x1 tensor to the `self.bert_model`\n","        #TODO: Note the Bert model will return a tuple, you only need the first item (which are the hidden states from the last layer of Bert) in the tuple for this task.\n","        #TODO: documentation for the Bert model can be found here: https://huggingface.co/transformers/model_doc/bert.html\n","        #TODO: the result should have shape (batch_size, seq_size, 768)\n","        x2 = self.bert_model(x1)\n","        #TODO: Extract the first time step hidden state from all the hidden states.\n","        #TODO: Pass the first time step hidden state through the `self.output` linear layer\n","        #TODO: Pass the output of the linear layer through a sigmoid function and name the result `out`.\n","        #TODO: `out` should have the shape (batch_size, 1)\n","        out = torch.sigmoid(self.output(x2[0][:,-1,:].squeeze(1)))\n","\n","        \n","        \n","        pred = out.clone().detach()\n","        pred[pred >= 0.5] = 1\n","        pred[pred < 0.5] = 0\n","        return out, pred"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V5vTTWrcqDgB","colab_type":"code","outputId":"355b27ad-9f49-4bae-8dcf-116c50f12db5","executionInfo":{"status":"ok","timestamp":1579129836040,"user_tz":-480,"elapsed":655610,"user":{"displayName":"Mou Zhang","photoUrl":"","userId":"02569161532640255187"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["bert_model = BERTClassifier()\n","bert_model = bert_model.cuda()\n","print(bert_model, '\\ncontains', sum([p.numel() for p in bert_model.parameters() if p.requires_grad]), 'parameters')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 492/492 [00:00<00:00, 473736.82B/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 267967963/267967963 [00:03<00:00, 72310751.52B/s]\n"],"name":"stderr"},{"output_type":"stream","text":["BERTClassifier(\n","  (dropout_layer): Dropout(p=0.1, inplace=False)\n","  (loss): BCELoss()\n","  (output): Linear(in_features=768, out_features=1, bias=True)\n","  (bert_model): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0): TransformerBlock(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (1): TransformerBlock(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (2): TransformerBlock(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (3): TransformerBlock(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (4): TransformerBlock(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (5): TransformerBlock(\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n",") \n","contains 66363649 parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0erCS_yoqMlf","colab_type":"code","outputId":"9d6c01b9-2a7b-4937-8b55-ddaf904e791b","executionInfo":{"status":"ok","timestamp":1579130522612,"user_tz":-480,"elapsed":1342170,"user":{"displayName":"Mou Zhang","photoUrl":"","userId":"02569161532640255187"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["bert_model = train(bert_model, train_corpus, dev_corpus, 3) # takes ~1 hour"],"execution_count":17,"outputs":[{"output_type":"stream","text":["epoch: 0 time elapsed: 220.21\n","train loss: 0.4207 train acc: 0.8011\n","  dev loss: 0.3455   dev acc: 0.8415\n","epoch: 1 time elapsed: 449.09\n","train loss: 0.2932 train acc: 0.8771\n","  dev loss: 0.2984   dev acc: 0.8765\n","epoch: 2 time elapsed: 678.29\n","train loss: 0.2231 train acc: 0.9122\n","  dev loss: 0.3167   dev acc: 0.8734\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TbstmfEhckWl","colab_type":"code","outputId":"d4aee55c-7fbf-460e-b9d7-d63342b77f2b","executionInfo":{"status":"ok","timestamp":1579130543338,"user_tz":-480,"elapsed":1362885,"user":{"displayName":"Mou Zhang","photoUrl":"","userId":"02569161532640255187"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["evaluate(bert_model, test_corpus)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Predictions:\n","Avg acc: 0.8684\n"],"name":"stdout"}]}]}